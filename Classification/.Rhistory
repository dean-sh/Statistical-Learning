attach(df)
summary(df)
pairs(df)
plot(age~chd)
plot(chd~agegrp)
df$id = NULL
chd = factor(chd)
smp_size = floor(0.7 * nrow(df))
set.seed(42)
train_index = sample(seq_len(nrow(df)), size = smp_size) # sample from seq of row numbers
train = df[train_index, ]
test = df[-train_index, ]
colnames(df)
logReg = glm(chd ~ age , data = train, family = binomial)
summary(logReg)
CM
TP
FP
TP = CM[2,2]
FN = CM[2,1]
TN = CM[1,1]
FP = CM[1,2]
ConfusionMatrixStatistics(TP, TN, FP, FN)
CM = ConfusionMatrixStatistics(TP, TN, FP, FN)
CM
emptymodel = glm(data = df, defult~1, family= binomial)
emptymodel = glm(data = df, default~1, family= binomial)
emptymodel = glm(data = df, chd~1, family= binomial)
lrtest(emptymodel,logReg)
emptymodel = glm(train = df, chd~1, family= binomial)
emptymodel = glm(data = train, chd~1, family= binomial)
lrtest(emptymodel,logReg)
install.packages("pROC")
library(pROC)
roc(response = test$chd, predictor = test.prediction, auc = T, plot = T)
roc(response = test$chd, predictor = test.prediction, auc = T, plot = T)
#            SIMPLE LOGISTIC REGRESSION             #
#Heart Disease Classification by Age
############################################
df = read.csv("chdage.csv")
attach(df)
summary(df)
pairs(df)
plot(age~chd)
plot(chd~agegrp)
df$id = NULL
chd = factor(chd)
############################################
#choosing training set and testing set
smp_size = floor(0.7 * nrow(df))
set.seed(42)
train_index = sample(seq_len(nrow(df)), size = smp_size) # sample from seq of row numbers
train = df[train_index, ]
test = df[-train_index, ]
colnames(df)
logReg = glm(chd ~ age , data = train, family = binomial)
summary(logReg)
#Test Prediction
test.prediction = predict(object = logReg, newdata = test, type = 'response')
predictions = rep("No", length(test$chd))
predictions[test.prediction>0.5] = "Yes" #Cutoff probability
CM =  table(predictions, test$chd) #Confusion Matrix
source("Confusion Matrix.R")
TP = CM[2,2]
FN = CM[2,1]
TN = CM[1,1]
FP = CM[1,2]
CM = ConfusionMatrixStatistics(TP, TN, FP, FN)
library(ggplot2)
plot(test$age, test$chd, pch = 16, xlab = "age", ylab = "CHD")
curve(test.prediction, add=TRUE)
plot(age, chd)
levels(test$chd) = c(0,1)
test$chd = as.numeric(test$chd)
ggplot(test) +
geom_point(aes(x=test$age, test$chd)) +
geom_point(aes(x=test$age, y=test.prediction), color="blue")
summary(logReg)
#plot(test$age,test.prediction,  col="blue")
##############################################
library(lmtest)
emptymodel = glm(data = train, chd~1, family= binomial)
lrtest(emptymodel,logReg)
library(pROC)
roc(response = test$chd, predictor = test.prediction, auc = T, plot = T)
#            SIMPLE LOGISTIC REGRESSION             #
#Heart Disease Classification by Age
############################################
df = read.csv("chdage.csv")
attach(df)
summary(df)
pairs(df)
plot(age~chd)
plot(chd~agegrp)
df$id = NULL
chd = factor(chd)
############################################
#choosing training set and testing set
smp_size = floor(0.7 * nrow(df))
set.seed(42)
train_index = sample(seq_len(nrow(df)), size = smp_size) # sample from seq of row numbers
train = df[train_index, ]
test = df[-train_index, ]
colnames(df)
logReg = glm(chd ~ age , data = train, family = binomial)
summary(logReg)
#Test Prediction
test.prediction = predict(object = logReg, newdata = test, type = 'response')
predictions = rep("No", length(test$chd))
predictions[test.prediction>0.5] = "Yes" #Cutoff probability
CM =  table(predictions, test$chd) #Confusion Matrix
source("Confusion Matrix.R")
TP = CM[2,2]
FN = CM[2,1]
TN = CM[1,1]
FP = CM[1,2]
CM = ConfusionMatrixStatistics(TP, TN, FP, FN)
library(ggplot2)
plot(test$age, test$chd, pch = 16, xlab = "age", ylab = "CHD")
curve(test.prediction, add=TRUE)
plot(age, chd)
levels(test$chd) = c(0,1)
test$chd = as.numeric(test$chd)
ggplot(test) +
geom_point(aes(x=test$age, test$chd)) +
geom_point(aes(x=test$age, y=test.prediction), color="blue")
summary(logReg)
#plot(test$age,test.prediction,  col="blue")
##############################################
library(lmtest)
emptymodel = glm(data = train, chd~1, family= binomial)
lrtest(emptymodel,logReg)
library(pROC)
roc(response = test$chd, predictor = test.prediction, auc = T, plot = T)
test$chd
test.prediction
plot(test$age,test.prediction,  col="blue")
roc(response = test$chd, predictor = test.prediction, auc = T, plot = T)
lrtest(emptymodel,logReg)
lr.test = lrtest(emptymodel,logReg)
roc(response = train$chd, predictor = test.prediction, auc = T, plot = T)
lr.test
View(lr.test)
lr.test$LogLik[0]
lr.test$LogLik[1]
LR.Statistic = -2*(lr.test$LogLik[1]-lr.test$LogLik[2])
logReg$null.deviance-logReg$deviance
#OR
LR.Statistic = logReg$null.deviance-logReg$deviance
roc(response = test$chd, predictor = test.prediction, auc = T, plot = T)
logReg2 = glm(chd ~ age , data = df, family = binomial)
test.prediction = predict(object = logReg2, type = 'response')
test.prediction2 = predict(object = logReg2, type = 'response')
roc(response = df$chd, predictor = test.prediction2, auc = T, plot = T)
logReg1 = glm(chd ~ age , data = train, family = binomial)
test.prediction2 = predict(object = logReg1, type = 'response')
test.prediction1 = predict(object = logReg1, type = 'response')
roc(response = test$chd, predictor = test.prediction1, auc = T, plot = T)
logReg1 = glm(chd ~ age , data = train, family = binomial)
test.prediction1 = predict(object = logReg1, type = 'response')
roc(response = test$chd, predictor = test.prediction1, auc = T, plot = T)
test.prediction1 = predict(object = logReg1, newdata = test, type = 'response')
roc(response = test$chd, predictor = test.prediction1, auc = T, plot = T)
LR.Pvalue = 1- pchisq(LR.Statistic)
LR.Pvalue
LR.Pvalue = 1- pchisq(LR.Statistic)
LR.Pvalue = 1- pchisq(LR.Statistic,1)
install.packages("rattle")
library(rattle)
install.packages("RGtk2")
install.packages("GTK")
rattle()
rattle()
install.packages(ISLR)
install.packages("ISLR")
library(ISLR)
df = read.csv("auto.csv")
attach(df)
mgp.mean = mean(df$mpg)
df$binaryMpg = rep(0, length(df$mpg))
df$binaryMpg[df$mpg>mgp.mean] = 1
source("Descriptive Stats.R")
DescStats(mpg)
DescStats(displacement)
plot(binaryMpg)
attach(df)
plot(binaryMpg)
plot(binaryMpg~horsepower)
View(df)
plot(binaryMpg~displacement)
boxplot(binaryMpg~displacement)
plot(binaryMpg~weight)
library(corrplot)
library(ggcorrplot)
ggcorrplot(df)
cor.test(df, method = "spearman")
cor.test(df, df, method = "spearman")
cor.test(binaryMpg, mpg, method = "spearman")
cor.test(binaryMpg, horsepower, method = "spearman")
library(ggplot2)
library(reshape2)
df.m <- melt(df, id.var = "binaryMpg")
View(df)
View(df.m)
ggplot(df.m, aes(x=binaryMpg, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y =mean, geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
ggplot(df.m,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable, scales="fixed")
View(df.m)
View(df.m)
View(df.m)
df.m <- melt(df, id.var = "binaryMpg")
View(df)
library(dplyr)
df.m = flight %>% select(-name, -`NA`)
df.m <- melt(df.m, id.var = "binaryMpg")
ggplot(df.m, aes(x=binaryMpg, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y =mean, geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
summarise(df)
summarise(df)
summarise(df.m)
summary(df)
names(Filter(is.factor, df))
df.m = flight %>% select(-name)
df.m = df %>% select(-name)
View(df.m)
View(df.m)
View(df.m)
df.m <- melt(df.m, id.var = "binaryMpg")
ggplot(df.m, aes(x=binaryMpg, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y =mean, geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
ggplot(df.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y =mean, geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable)
ggplot(df.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
names(Filter(is.factor, df.m))
df))
names(Filter(is.factor, df))
ggplot(df.m,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable, scales="fixed")
ggplot(df.m,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable, scales="free")
df$binaryMpg = factor(df$binaryMpg)
names(Filter(is.factor, df))
df.m = df %>% select(-name)
df.m <- melt(df.m, id.var = "binaryMpg")
ggplot(df.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
ggplot(df.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable)
ggplot(df.m,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable, scales="free")
ggplot(df.m,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable, scales="fixed")
ggplot(df.m,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable)
ggplot(df.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=Direction))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
ggplot(df.m, aes(x=variable, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
ggplot(df.m, aes(x=binaryMpg, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
value
ggplot(df.m,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable, scales="free")
smp_size = floor(0.7 * nrow(df))
set.seed(42)
train_index = sample(seq_len(nrow(df)), size = smp_size) # sample from seq of row numbers
train = df[train_index, ]
test = df[-train_index, ]
colnames(df)
############################################
#             Train-Test Split             #
############################################
df = df %>% select(-name)
smp_size = floor(0.7 * nrow(df))
set.seed(42)
train_index = sample(seq_len(nrow(df)), size = smp_size) # sample from seq of row numbers
train = df[train_index, ]
test = df[-train_index, ]
colnames(df)
############################################
#             Train-Test Split             #
############################################
df = df %>% select(-name, mpg)
############################################
#             Train-Test Split             #
############################################
df = df %>% select(-name, -mpg)
############################################
#             Train-Test Split             #
############################################
df = df %>% select(-c(name, mpg))
############################################
#             Train-Test Split             #
############################################
df = df %>% select(-name, -mpg)
colnames(df)
df = read.csv("auto.csv")
mgp.mean = mean(df$mpg)
df$binaryMpg = rep(0, length(df$mpg))
df$binaryMpg[df$mpg>mgp.mean] = 1
df$binaryMpg = factor(df$binaryMpg)
attach(df)
############################################
#             Train-Test Split             #
############################################
df = df %>% select(-name, -mpg)
smp_size = floor(0.7 * nrow(df))
set.seed(42)
train_index = sample(seq_len(nrow(df)), size = smp_size) # sample from seq of row numbers
train = df[train_index, ]
test = df[-train_index, ]
colnames(df)
lgr = glm(binaryMpg~. , data = train, family = binomial)
summary(lgr)
df.c = df
df.c$cylinders = factor(df.c$cylinders)
df.c$year = factor(df.c$year)
df.c$origin = factor(df.c$origin)
train.c = df.c[train_index, ]
test.c = df.c[-train_index, ]
lgr.c = glm(binaryMpg~. , data = train.c, family = binomial)
summary(lgr.c)
lgr.c = glm(binaryMpg~cylinders+displacement+weight+horsepower , data = train.c, family = binomial)
summary(lgr.c)
lgr.c = glm(binaryMpg~displacement+weight+horsepower , data = train.c, family = binomial)
summary(lgr.c)
lgr.c = glm(binaryMpg~year+displacement+weight+horsepower , data = train.c, family = binomial)
summary(lgr.c)
lgr.c = glm(binaryMpg~displacement+weight+horsepower , data = train.c, family = binomial)
lgr.c = glm(binaryMpg~origin+displacement+weight+horsepower , data = train.c, family = binomial)
summary(lgr.c)
View(df.m)
df.c <- melt(df.c, id.var = "binaryMpg")
ggplot(df.c, aes(x=binaryMpg, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
ggplot(df.c,aes(x=value, fill=binaryMpg)) +
geom_density(alpha = 0.5)+
facet_wrap(~variable, scales="free")
ggplot(df.m, aes(x=binaryMpg, y=value)) +
geom_boxplot(aes(fill=binaryMpg))+
stat_summary(fun.y = "mean", geom = "point", shape= 23, size= 3, fill= "red") +
facet_wrap(~variable, scales="free")
lgr = glm(binaryMpg~origin+displacement+weight+horsepower , data = train, family = binomial)
summary(lgr)
lgr.c = glm(binaryMpg~origin+displacement+weight+horsepower+weight*horsepower , data = train.c, family = binomial)
lgr = glm(binaryMpg~origin+displacement+weight+horsepower +weight*horsepower , data = train, family = binomial)
summary(lgr)
lgr = glm(binaryMpg~origin+displacement+weight+horsepower +weight/horsepower , data = train, family = binomial)
summary(lgr.c)
lgr = glm(binaryMpg~displacement+weight+horsepower , data = train, family = binomial)
summary(lgr)
lgr = glm(binaryMpg~displacement+weight+horsepower+cylinders , data = train, family = binomial)
summary(lgr)
lgr.c = glm(binaryMpg~displacement+weight+horsepower+cylinders , data = train.c, family = binomial)
summary(lgr.c)
plot(mpg~cylinders)
plot(binaryMpg~cylinders)
test.prediction = predict(object = lgr, newdata = test, type = 'response')
contrasts(df$binaryMpg)
cutoff=0.5
predictions = rep(0, length(test$binaryMpg))
predictions[test.prediction>cutoff] = 1 #Cutoff probability
print(table(predictions, test$Direction))
print(table(predictions, test$binaryMpg))
CM = table(predictions, test$binaryMpg)
source("Confusion Matrix.R")
TP = CM[2,2]
FN = CM[2,1]
TN = CM[1,1]
FP = CM[1,2]
Metrics = ConfusionMatrixStatistics(TP, TN, FP, FN) #Statistics - Accuracy, Recall, Precs.
Metrics
plot(lgr)
test.prediction.c = predict(object = lgr.c, newdata = test.c, type = 'response')
predictions.c = rep(0, length(test.c$binaryMpg))
predictions.c[test.prediction.c>cutoff] = 1 #Cutoff probability
CM = table(predictions.c, test.c$binaryMpg)
CM
CM.c = table(predictions.c, test.c$binaryMpg)
TP.c = CM.c[2,2]
FN.c = CM.c[2,1]
TN.c = CM.c[1,1]
FP.c = CM.c[1,2]
ConfusionMatrixStatistics(TP.c, TN.c, FP.c, FN.c)
Metrics - ConfusionMatrixStatistics(TP.c, TN.c, FP.c, FN.c)
lgr.c = glm(binaryMpg~displacement+weight+horsepower+cylinders , data = train.c, family = binomial)
predictions.c = rep(0, length(test.c$binaryMpg))
predictions.c[test.prediction.c>cutoff] = 1 #Cutoff probability
CM.c = table(predictions.c, test.c$binaryMpg)
source("Confusion Matrix.R")
TP = CM[2,2]
FN = CM[2,1]
TN = CM[1,1]
FP = CM[1,2]
TP.c = CM.c[2,2]
FN.c = CM.c[2,1]
TN.c = CM.c[1,1]
FP.c = CM.c[1,2]
Metrics = ConfusionMatrixStatistics(TP, TN, FP, FN) #Statistics - Accuracy, Recall, Precs.
Metrics.c = ConfusionMatrixStatistics(TP.c, TN.c, FP.c, FN.c)
Metrics.c
Metrics
Metrics.c
Metrics
Metrics.c
CM.c
CM
library(lmtest)
emptymodel = glm(data = train, Direction~1, family= binomial)
lr.test = lrtest(emptymodel,lgr)
LR.Statistic = -2*(lr.test$LogLik[1]-lr.test$LogLik[2])
#OR
LR.Statistic = lgr$null.deviance-lgr$deviance
LR.Pvalue = 1- pchisq(LR.Statistic,1)
LR.Pvalue
LR.Statistic
LR.Statistic
-2*(lr.test$LogLik[1]-lr.test$LogLik[2])
lr.test = lrtest(emptymodel,lgr)
library(lmtest)
install.packages("lmtest")
library(lmtest)
emptymodel = glm(data = train, Direction~1, family= binomial)
lr.test = lrtest(emptymodel,lgr)
LR.Statistic = -2*(lr.test$LogLik[1]-lr.test$LogLik[2])
#OR
LR.Statistic = lgr$null.deviance-lgr$deviance
LR.Statistic
lr.test
lr.test = lrtest(emptymodel,lgr)
emptymodel = glm(data = train, Direction~1, family= binomial)
emptymodel = glm(data = train, binaryMpg~1, family= binomial)
lr.test = lrtest(emptymodel,lgr)
LR.Statistic = -2*(lr.test$LogLik[1]-lr.test$LogLik[2])
#OR
LR.Statistic = lgr$null.deviance-lgr$deviance
LR.Pvalue = 1- pchisq(LR.Statistic,1)
LR.Pvalue
lr.test
LR.Statistic
LR.Pvalue
library(pROC)
install.packages("pROC")
library(pROC)
roc(response = test$binaryMpg, predictor = test.prediction, auc = T, plot = T)
#ROC over test data only:
roc(response = test.c$binaryMpg, predictor = test.prediction.c, auc = T, plot = T)
library(class)
View(df)
depvars =  names(train) %in% c("mpg", "origin")
train.x = train[!depvars]
test.x = test[!depvars]
train.y = train$binaryMpg
test.y = test$binaryMpg
train.x = scale(train.x)
test.x = scale(test.x)
kvec = seq(1, 250, by=1)
probs = c()
for (k in kvec){
prob = 100 * sum(test.y == knn(train.x, test.x, train.y, k))/length(test.y)
probs = append(probs, prob)
}
kvec = data.frame(kvec,probs)
plot(kvec)
depvars =  names(train) %in% c("names", "mpg", "origin")
train.x = train[!depvars]
test.x = test[!depvars]
train.y = train$binaryMpg
test.y = test$binaryMpg
train.x = scale(train.x)
test.x = scale(test.x)
kvec = seq(1, 250, by=1)
probs = c()
for (k in kvec){
prob = 100 * sum(test.y == knn(train.x, test.x, train.y, k))/length(test.y)
probs = append(probs, prob)
}
kvec = data.frame(kvec,probs)
plot(kvec)
best.k = kvec[which.max(kvec$probs),]
best.k
